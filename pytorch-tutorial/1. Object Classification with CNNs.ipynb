{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Classification with CNNs\n",
    "\n",
    "Object classification in images with convolutional neural networks (CNNs) is arguably what ignited the field of deep learning (or re-ignited the field of neural networks). Even now, deep learning shines in supervised learning settings when there is plenty of data, and so we'll look at a fairly standard pipeline based on object classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "MNIST\n",
    "CIFAR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "data_path = os.path.join(os.path.expanduser('~'), '.torch', 'datasets', 'mnist')\n",
    "train_data = datasets.MNIST(data_path, train=True, download=True,\n",
    "                            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "test_data = datasets.MNIST(data_path, train=False,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.prelu1 = nn.PReLU(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.prelu2 = nn.PReLU(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.prelu3 = nn.PReLU(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.prelu4 = nn.PReLU(64)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512, bias=False)\n",
    "        self.dp1 = nn.Dropout()\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.prelu5 = nn.PReLU(512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.bn1(self.conv1(x)))\n",
    "        x = self.prelu2(self.bn2(self.conv2(x)))\n",
    "        x = self.prelu3(self.bn3(self.conv3(x)))\n",
    "        x = self.prelu4(self.bn4(self.conv4(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.prelu5(self.bn5(self.dp1(self.fc1(x))))\n",
    "        return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.290889263153076\n",
      "10 1.3428609371185303\n",
      "20 0.9157310128211975\n",
      "30 0.6610764265060425\n",
      "40 0.6482018232345581\n",
      "50 0.44249194860458374\n",
      "60 0.534844696521759\n",
      "70 0.4320223927497864\n",
      "80 0.23216548562049866\n",
      "90 0.29437491297721863\n",
      "100 0.3193305730819702\n",
      "110 0.29913055896759033\n",
      "120 0.20930927991867065\n",
      "130 0.19264532625675201\n",
      "140 0.21241307258605957\n",
      "150 0.3589246869087219\n",
      "160 0.19412192702293396\n",
      "170 0.22808325290679932\n",
      "180 0.22743552923202515\n",
      "190 0.11307939887046814\n",
      "200 0.125473290681839\n",
      "210 0.1475408375263214\n",
      "220 0.1659116894006729\n",
      "230 0.1664218306541443\n",
      "240 0.08588605374097824\n",
      "250 0.07279990613460541\n",
      "260 0.10554340481758118\n",
      "270 0.07441550493240356\n",
      "280 0.1585436463356018\n",
      "290 0.17889198660850525\n",
      "300 0.14697301387786865\n",
      "310 0.09552984684705734\n",
      "320 0.10998660326004028\n",
      "330 0.12195517867803574\n",
      "340 0.06838690489530563\n",
      "350 0.06499971449375153\n",
      "360 0.09424880146980286\n",
      "370 0.08306574076414108\n",
      "380 0.041483305394649506\n",
      "390 0.10213358700275421\n",
      "400 0.09162051975727081\n",
      "410 0.23035699129104614\n",
      "420 0.07529512792825699\n",
      "430 0.11545237898826599\n",
      "440 0.081893190741539\n",
      "450 0.03136395663022995\n",
      "460 0.09576249867677689\n",
      "470 0.06011810153722763\n",
      "480 0.14635547995567322\n",
      "490 0.09789358079433441\n",
      "500 0.08689290285110474\n",
      "510 0.03431911766529083\n",
      "520 0.10837452113628387\n",
      "530 0.0688038170337677\n",
      "540 0.12009463459253311\n",
      "550 0.037699151784181595\n",
      "560 0.16553157567977905\n",
      "570 0.025957606732845306\n",
      "580 0.05363950505852699\n",
      "590 0.06666853278875351\n",
      "600 0.07542912662029266\n",
      "610 0.1454237997531891\n",
      "620 0.09518682211637497\n",
      "630 0.051941972225904465\n",
      "640 0.04903915524482727\n",
      "650 0.10976912826299667\n",
      "660 0.0441926009953022\n",
      "670 0.0618133544921875\n",
      "680 0.08814989775419235\n",
      "690 0.07204855233430862\n",
      "700 0.042234376072883606\n",
      "710 0.03774914890527725\n",
      "720 0.05722462385892868\n",
      "730 0.03430260345339775\n",
      "740 0.04414323344826698\n",
      "750 0.03622662276029587\n",
      "760 0.06445791572332382\n",
      "770 0.03364652395248413\n",
      "780 0.07798327505588531\n",
      "790 0.040804095566272736\n",
      "800 0.03942703455686569\n",
      "810 0.13910424709320068\n",
      "820 0.13032835721969604\n",
      "830 0.05236700177192688\n",
      "840 0.037625133991241455\n",
      "850 0.04241839423775673\n",
      "860 0.10143943876028061\n",
      "870 0.07461658865213394\n",
      "880 0.05349396541714668\n",
      "890 0.06783512979745865\n",
      "900 0.03486201539635658\n",
      "910 0.12131785601377487\n",
      "920 0.0568075031042099\n",
      "930 0.03913824260234833\n"
     ]
    }
   ],
   "source": [
    "optimiser = optim.Adam(model.parameters(), lr=5e-4)\n",
    "model.train()\n",
    "train_losses = []\n",
    "\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "  optimiser.zero_grad()\n",
    "  y_hat = model(x)\n",
    "  loss = F.nll_loss(y_hat, y)\n",
    "  loss.backward()\n",
    "  train_losses.append(loss.item())\n",
    "  optimiser.step()\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(i, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9882 0.0378669185757637\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for x, y in test_loader:\n",
    "    y_hat = model(x)\n",
    "    test_loss += F.nll_loss(y_hat, y, reduction='sum').item()\n",
    "    pred = y_hat.argmax(1, keepdim=True)\n",
    "    correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_data)\n",
    "acc = correct / len(test_data)\n",
    "print(acc, test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
